{
  "num_clients": 20,
  "update_frequency": 1,
  "initial_distribution": [
    0.1,
    0.1,
    0.8
  ],
  "simulator_config": {
    "replicator": {
      "learning_rate": 0.05,
      "noise_level": 0.02
    },
    "honest_quality": 0.9,
    "withholding_quality": 0.5,
    "adversarial_quality": 0.1
  },
  "reward_mechanism": "adaptive",
  "reward_config": {
    "initial_pool": 5.0,
    "learning_rate": 0.0,
    "accuracy_threshold": 0.5,
    "strategy_weights": [
      1.0,
      1.0,
      1.0
    ]
  },
  "cost_mechanism": "computational",
  "cost_config": {
    "base_costs": {
      "honest": 10.0,
      "withholding": 5.0,
      "adversarial": 1.0
    },
    "scaling_factor": 0.0
  },
  "punishment_mechanism": "reputation",
  "punishment_config": {
    "memory_factor": 0.8,
    "reputation_threshold": 0.5,
    "max_penalty": 20.0
  },
  "description": "Adaptive Penalty scenario for Chapter 5 (Section 1): Same static reward and cost structure as baseline, but with a reputation-based punishment mechanism that tracks client behavior over time. Clients with low reputation receive higher penalties, allowing honest strategies to eventually dominate as adversarial agents accumulate negative reputation scores."
} 