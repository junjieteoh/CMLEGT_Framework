{
  "num_clients": 20,
  "update_frequency": 1,
  "initial_distribution": [
    0.14285714285714285,
    0.5714285714285714,
    0.2857142857142858
  ],
  "simulator_config": {
    "replicator": {
      "learning_rate": 0.05,
      "noise_level": 0.02
    },
    "honest_quality": 0.9,
    "withholding_quality": 0.5,
    "adversarial_quality": 0.1
  },
  "reward_mechanism": "adaptive",
  "reward_config": {
    "initial_pool": 5.0,
    "learning_rate": 0.0,
    "accuracy_threshold": 0.5,
    "strategy_weights": [
      1.0,
      1.0,
      1.0
    ]
  },
  "cost_mechanism": "computational",
  "cost_config": {
    "base_costs": {
      "honest": 10.0,
      "withholding": 5.0,
      "adversarial": 1.0
    },
    "scaling_factor": 0.0
  },
  "punishment_mechanism": "none",
  "punishment_config": {
    "penalty_strength": 0.0,
    "detection_threshold": 1.0
  },
  "description": "Baseline scenario for Chapter 5 (Section 1): Static reward with strategy-dependent costs that favor free-riding (adversarial strategy). This configuration demonstrates how adversarial strategy dominates when there's no penalty mechanism, as the net payoff for adversarial agents remains higher than for honest contributors.",
  "strategy_distribution": {
    "honest": 0.14285714285714285,
    "withholding": 0.5714285714285714,
    "adversarial": 0.2857142857142858
  }
}